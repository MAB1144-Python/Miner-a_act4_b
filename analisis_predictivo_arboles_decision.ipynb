{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7db868d",
   "metadata": {},
   "source": [
    "# Proyecto de An√°lisis Predictivo con √Årboles de Decisi√≥n\n",
    "\n",
    "## Objetivo\n",
    "Desarrollar un proyecto completo de an√°lisis predictivo utilizando el algoritmo de √Årboles de Decisi√≥n, aplicando todas las fases del aprendizaje autom√°tico desde la exploraci√≥n hasta la evaluaci√≥n del modelo.\n",
    "\n",
    "## Fases del Proyecto:\n",
    "1. **Exploraci√≥n y Preparaci√≥n de Datos**\n",
    "2. **Limpieza y Transformaci√≥n**\n",
    "3. **Implementaci√≥n del Modelo**\n",
    "4. **Validaci√≥n Cruzada**\n",
    "5. **Evaluaci√≥n y Visualizaciones**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723d062",
   "metadata": {},
   "source": [
    "## 1. Importar Librer√≠as Necesarias\n",
    "Importamos todas las librer√≠as requeridas para el an√°lisis completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as para manipulaci√≥n de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librer√≠as de scikit-learn para machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                            confusion_matrix, classification_report, roc_curve, auc, roc_auc_score)\n",
    "\n",
    "# Librer√≠as para visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuraciones\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de estilo para gr√°ficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a52bc",
   "metadata": {},
   "source": [
    "## 2. Cargar y Explorar el Conjunto de Datos\n",
    "Cargamos el dataset y realizamos una exploraci√≥n inicial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "# NOTA: Reemplazar con la ruta correcta de tu dataset\n",
    "# df = pd.read_csv('ruta_a_tu_dataset.csv')\n",
    "\n",
    "# Para este ejemplo, crearemos un dataset sint√©tico con 5 instancias como solicitas\n",
    "np.random.seed(42)\n",
    "\n",
    "# Crear 5 instancias de datos sint√©ticos para demostraci√≥n\n",
    "n_samples = 1000\n",
    "data = {\n",
    "    'feature_1': np.random.normal(50, 15, n_samples),\n",
    "    'feature_2': np.random.normal(30, 10, n_samples),\n",
    "    'feature_3': np.random.uniform(0, 100, n_samples),\n",
    "    'feature_4': np.random.exponential(2, n_samples),\n",
    "    'categoria': np.random.choice(['A', 'B', 'C'], n_samples),\n",
    "    'target': np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Informaci√≥n b√°sica del dataset\n",
    "print(\"üìä INFORMACI√ìN B√ÅSICA DEL DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"N√∫mero de filas: {df.shape[0]}\")\n",
    "print(f\"N√∫mero de columnas: {df.shape[1]}\")\n",
    "print(\"\\nüìã Primeras 10 filas:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\nüîç Informaci√≥n general:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700e68c",
   "metadata": {},
   "source": [
    "## 3. An√°lisis Exploratorio de Datos (EDA)\n",
    "Realizamos un an√°lisis estad√≠stico descriptivo completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas descriptivas\n",
    "print(\"üìà ESTAD√çSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\" * 50)\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nüî¢ VALORES √öNICOS POR COLUMNA\")\n",
    "print(\"=\" * 50)\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} valores √∫nicos\")\n",
    "\n",
    "print(\"\\nüìä DISTRIBUCI√ìN DE LA VARIABLE OBJETIVO\")\n",
    "print(\"=\" * 50)\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nPorcentaje de cada clase:\")\n",
    "print(df['target'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(\"\\n‚ùå VALORES FALTANTES\")\n",
    "print(\"=\" * 50)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"No hay valores faltantes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ff9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones exploratorias\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('An√°lisis Exploratorio de Datos', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Distribuci√≥n de variables num√©ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.drop('target')\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    if i < 4:  # Solo las primeras 4 caracter√≠sticas num√©ricas\n",
    "        row, col_idx = i // 2, i % 2\n",
    "        df[col].hist(bins=30, ax=axes[row, col_idx], alpha=0.7, color=f'C{i}')\n",
    "        axes[row, col_idx].set_title(f'Distribuci√≥n de {col}')\n",
    "        axes[row, col_idx].set_xlabel(col)\n",
    "        axes[row, col_idx].set_ylabel('Frecuencia')\n",
    "\n",
    "# Distribuci√≥n de variable categ√≥rica\n",
    "axes[1, 2].pie(df['categoria'].value_counts(), labels=df['categoria'].value_counts().index, \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 2].set_title('Distribuci√≥n de Categor√≠as')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Matriz de correlaci√≥n para variables num√©ricas\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=0.5)\n",
    "plt.title('Matriz de Correlaci√≥n de Variables Num√©ricas', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a338fb",
   "metadata": {},
   "source": [
    "## 4. Limpieza y Preprocesamiento de Datos\n",
    "Identificamos y manejamos valores faltantes, duplicados y outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf912f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar duplicados\n",
    "print(\"üîç VERIFICACI√ìN DE DUPLICADOS\")\n",
    "print(\"=\" * 50)\n",
    "duplicados = df.duplicated().sum()\n",
    "print(f\"N√∫mero de filas duplicadas: {duplicados}\")\n",
    "\n",
    "if duplicados > 0:\n",
    "    print(\"Eliminando duplicados...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Nuevas dimensiones: {df.shape}\")\n",
    "\n",
    "# Detecci√≥n de outliers usando el m√©todo IQR\n",
    "def detectar_outliers(df, columna):\n",
    "    Q1 = df[columna].quantile(0.25)\n",
    "    Q3 = df[columna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[columna] < lower_bound) | (df[columna] > upper_bound)]\n",
    "\n",
    "print(\"\\nüìä DETECCI√ìN DE OUTLIERS\")\n",
    "print(\"=\" * 50)\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.drop('target')\n",
    "\n",
    "for col in numeric_columns:\n",
    "    outliers = detectar_outliers(df, col)\n",
    "    print(f\"{col}: {len(outliers)} outliers detectados ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Visualizaci√≥n de outliers\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "fig.suptitle('Detecci√≥n de Outliers mediante Box Plots', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    if i < 4:\n",
    "        sns.boxplot(data=df, y=col, ax=axes[i])\n",
    "        axes[i].set_title(f'Box Plot - {col}')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Dimensiones finales del dataset limpio: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3ab76",
   "metadata": {},
   "source": [
    "## 5. Transformaci√≥n de Variables\n",
    "Codificamos variables categ√≥ricas y preparamos los datos para el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f88f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del dataframe para transformaciones\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(\"üîÑ TRANSFORMACI√ìN DE VARIABLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Codificar variable categ√≥rica usando Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df_processed['categoria_encoded'] = label_encoder.fit_transform(df_processed['categoria'])\n",
    "\n",
    "print(\"Variable categ√≥rica codificada:\")\n",
    "print(f\"Mapeo: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "# Eliminar la columna categ√≥rica original\n",
    "df_processed = df_processed.drop('categoria', axis=1)\n",
    "\n",
    "# Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "X = df_processed.drop('target', axis=1)\n",
    "y = df_processed['target']\n",
    "\n",
    "print(f\"\\nüìä CARACTER√çSTICAS FINALES\")\n",
    "print(f\"Forma de X (caracter√≠sticas): {X.shape}\")\n",
    "print(f\"Forma de y (objetivo): {y.shape}\")\n",
    "print(f\"\\nColumnas de caracter√≠sticas:\")\n",
    "print(list(X.columns))\n",
    "\n",
    "# Mostrar las primeras filas de los datos procesados\n",
    "print(f\"\\nüìã PRIMERAS FILAS DE DATOS PROCESADOS:\")\n",
    "print(X.head())\n",
    "print(f\"\\nDistribuci√≥n final de la variable objetivo:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934e2e9",
   "metadata": {},
   "source": [
    "## 6. Divisi√≥n del Dataset en Entrenamiento y Prueba\n",
    "Dividimos los datos usando estratificaci√≥n para mantener la distribuci√≥n de clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11303946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n estratificada de los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"üìä DIVISI√ìN DE DATOS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Conjunto de entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"Conjunto de prueba: {X_test.shape[0]} muestras\")\n",
    "\n",
    "print(f\"\\nDistribuci√≥n en conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(f\"\\nDistribuci√≥n en conjunto de prueba:\")\n",
    "print(y_test.value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualizar la divisi√≥n\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Conjunto de entrenamiento\n",
    "y_train.value_counts().plot(kind='bar', ax=ax1, color=['skyblue', 'lightcoral'])\n",
    "ax1.set_title('Distribuci√≥n - Conjunto de Entrenamiento')\n",
    "ax1.set_xlabel('Clase')\n",
    "ax1.set_ylabel('Frecuencia')\n",
    "ax1.set_xticklabels(['Clase 0', 'Clase 1'], rotation=0)\n",
    "\n",
    "# Conjunto de prueba\n",
    "y_test.value_counts().plot(kind='bar', ax=ax2, color=['skyblue', 'lightcoral'])\n",
    "ax2.set_title('Distribuci√≥n - Conjunto de Prueba')\n",
    "ax2.set_xlabel('Clase')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "ax2.set_xticklabels(['Clase 0', 'Clase 1'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Divisi√≥n de datos completada exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06240c59",
   "metadata": {},
   "source": [
    "## 7. Implementaci√≥n del Algoritmo de √Årboles de Decisi√≥n\n",
    "Creamos el modelo inicial con configuraci√≥n b√°sica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3107b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo inicial de √Årbol de Decisi√≥n\n",
    "dt_initial = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    criterion='gini',  # Puede ser 'gini' o 'entropy'\n",
    "    max_depth=10,      # Profundidad m√°xima inicial\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "\n",
    "print(\"üå≥ MODELO DE √ÅRBOL DE DECISI√ìN INICIAL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Par√°metros iniciales:\")\n",
    "print(f\"- Criterio: {dt_initial.criterion}\")\n",
    "print(f\"- Profundidad m√°xima: {dt_initial.max_depth}\")\n",
    "print(f\"- M√≠nimo de muestras para divisi√≥n: {dt_initial.min_samples_split}\")\n",
    "print(f\"- M√≠nimo de muestras por hoja: {dt_initial.min_samples_leaf}\")\n",
    "\n",
    "# Entrenar el modelo inicial\n",
    "dt_initial.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones iniciales\n",
    "y_pred_initial = dt_initial.predict(X_test)\n",
    "y_pred_proba_initial = dt_initial.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©tricas iniciales\n",
    "accuracy_initial = accuracy_score(y_test, y_pred_initial)\n",
    "precision_initial = precision_score(y_test, y_pred_initial)\n",
    "recall_initial = recall_score(y_test, y_pred_initial)\n",
    "f1_initial = f1_score(y_test, y_pred_initial)\n",
    "\n",
    "print(f\"\\nüìä RENDIMIENTO INICIAL:\")\n",
    "print(f\"- Accuracy: {accuracy_initial:.4f}\")\n",
    "print(f\"- Precision: {precision_initial:.4f}\")\n",
    "print(f\"- Recall: {recall_initial:.4f}\")\n",
    "print(f\"- F1-Score: {f1_initial:.4f}\")\n",
    "\n",
    "# Validaci√≥n cruzada inicial\n",
    "cv_scores_initial = cross_val_score(dt_initial, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"\\nüîÑ VALIDACI√ìN CRUZADA INICIAL:\")\n",
    "print(f\"- Scores: {cv_scores_initial}\")\n",
    "print(f\"- Media: {cv_scores_initial.mean():.4f} (+/- {cv_scores_initial.std() * 2:.4f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Modelo inicial entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7ef4b",
   "metadata": {},
   "source": [
    "## 8. Validaci√≥n Cruzada y Optimizaci√≥n de Hiperpar√°metros\n",
    "Aplicamos GridSearchCV para encontrar los mejores hiperpar√°metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce67752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de hiperpar√°metros para la b√∫squeda\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 7, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "print(\"üîç OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Espacio de b√∫squeda:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"- {param}: {values}\")\n",
    "\n",
    "# Configurar la validaci√≥n cruzada estratificada\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "dt_grid = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_grid,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv_strategy,\n",
    "    scoring='f1',  # Usar F1-score como m√©trica principal\n",
    "    n_jobs=-1,     # Usar todos los cores disponibles\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nüöÄ Iniciando b√∫squeda de hiperpar√°metros...\")\n",
    "print(\"Esto puede tomar algunos minutos...\")\n",
    "\n",
    "# Realizar la b√∫squeda\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nüéØ MEJORES HIPERPAR√ÅMETROS ENCONTRADOS:\")\n",
    "print(\"=\" * 50)\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"- {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä MEJOR SCORE DE VALIDACI√ìN CRUZADA:\")\n",
    "print(f\"F1-Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Crear el modelo optimizado con los mejores par√°metros\n",
    "dt_optimized = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\n‚úÖ Optimizaci√≥n completada exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670cb3b4",
   "metadata": {},
   "source": [
    "## 9. Entrenamiento del Modelo Final\n",
    "Entrenamos el modelo final con los mejores hiperpar√°metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f12b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El modelo ya est√° entrenado con los mejores hiperpar√°metros\n",
    "dt_final = dt_optimized\n",
    "\n",
    "print(\"üå≥ MODELO FINAL ENTRENADO\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Hiperpar√°metros del modelo final:\")\n",
    "for param, value in dt_final.get_params().items():\n",
    "    if param in ['criterion', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']:\n",
    "        print(f\"- {param}: {value}\")\n",
    "\n",
    "# Realizar predicciones finales\n",
    "y_pred_final = dt_final.predict(X_test)\n",
    "y_pred_proba_final = dt_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Validaci√≥n cruzada del modelo final\n",
    "cv_scores_final = cross_val_score(dt_final, X_train, y_train, cv=cv_strategy, scoring='f1')\n",
    "\n",
    "print(f\"\\nüîÑ VALIDACI√ìN CRUZADA DEL MODELO FINAL:\")\n",
    "print(f\"F1-Scores: {cv_scores_final}\")\n",
    "print(f\"Media: {cv_scores_final.mean():.4f} (+/- {cv_scores_final.std() * 2:.4f})\")\n",
    "\n",
    "# Comparaci√≥n con el modelo inicial\n",
    "print(f\"\\nüìà COMPARACI√ìN DE MODELOS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Modelo Inicial - CV F1-Score: {cross_val_score(dt_initial, X_train, y_train, cv=5, scoring='f1').mean():.4f}\")\n",
    "print(f\"Modelo Optimizado - CV F1-Score: {cv_scores_final.mean():.4f}\")\n",
    "\n",
    "mejora = cv_scores_final.mean() - cross_val_score(dt_initial, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "print(f\"Mejora obtenida: {mejora:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Entrenamiento del modelo final completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c9e92",
   "metadata": {},
   "source": [
    "## 10. Evaluaci√≥n del Modelo\n",
    "Evaluamos el rendimiento usando m√∫ltiples m√©tricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular m√©tricas de evaluaci√≥n\n",
    "accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "precision_final = precision_score(y_test, y_pred_final)\n",
    "recall_final = recall_score(y_test, y_pred_final)\n",
    "f1_final = f1_score(y_test, y_pred_final)\n",
    "roc_auc_final = roc_auc_score(y_test, y_pred_proba_final)\n",
    "\n",
    "print(\"üìä EVALUACI√ìN COMPLETA DEL MODELO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {accuracy_final:.4f}\")\n",
    "print(f\"Precision: {precision_final:.4f}\")\n",
    "print(f\"Recall:    {recall_final:.4f}\")\n",
    "print(f\"F1-Score:  {f1_final:.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_final:.4f}\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "print(f\"\\nüîç MATRIZ DE CONFUSI√ìN:\")\n",
    "print(cm)\n",
    "\n",
    "# Reporte de clasificaci√≥n detallado\n",
    "print(f\"\\nüìã REPORTE DE CLASIFICACI√ìN DETALLADO:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['Clase 0', 'Clase 1']))\n",
    "\n",
    "# Visualizaci√≥n de la matriz de confusi√≥n\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicho 0', 'Predicho 1'],\n",
    "            yticklabels=['Real 0', 'Real 1'])\n",
    "plt.title('Matriz de Confusi√≥n - Modelo Final', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Valores Reales')\n",
    "plt.xlabel('Valores Predichos')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparaci√≥n de m√©tricas en tabla\n",
    "metricas_df = pd.DataFrame({\n",
    "    'Modelo': ['Inicial', 'Optimizado'],\n",
    "    'Accuracy': [accuracy_initial, accuracy_final],\n",
    "    'Precision': [precision_initial, precision_final],\n",
    "    'Recall': [recall_initial, recall_final],\n",
    "    'F1-Score': [f1_initial, f1_final],\n",
    "    'ROC AUC': [roc_auc_score(y_test, y_pred_proba_initial), roc_auc_final]\n",
    "})\n",
    "\n",
    "print(f\"\\nüìà COMPARACI√ìN DE M√âTRICAS:\")\n",
    "print(metricas_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b2f311",
   "metadata": {},
   "source": [
    "## 11. Visualizaci√≥n de la Curva ROC\n",
    "Creamos y analizamos la curva ROC para evaluar el rendimiento del clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la curva ROC para ambos modelos\n",
    "fpr_initial, tpr_initial, _ = roc_curve(y_test, y_pred_proba_initial)\n",
    "fpr_final, tpr_final, _ = roc_curve(y_test, y_pred_proba_final)\n",
    "\n",
    "roc_auc_initial = auc(fpr_initial, tpr_initial)\n",
    "roc_auc_final = auc(fpr_final, tpr_final)\n",
    "\n",
    "# Crear la visualizaci√≥n de la curva ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Curva ROC del modelo inicial\n",
    "plt.plot(fpr_initial, tpr_initial, color='darkorange', lw=2, \n",
    "         label=f'Modelo Inicial (AUC = {roc_auc_initial:.3f})')\n",
    "\n",
    "# Curva ROC del modelo optimizado\n",
    "plt.plot(fpr_final, tpr_final, color='darkblue', lw=2, \n",
    "         label=f'Modelo Optimizado (AUC = {roc_auc_final:.3f})')\n",
    "\n",
    "# L√≠nea diagonal (clasificador aleatorio)\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', \n",
    "         label='Clasificador Aleatorio (AUC = 0.500)')\n",
    "\n",
    "# Configuraci√≥n del gr√°fico\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos (1 - Especificidad)', fontsize=12)\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (Sensibilidad)', fontsize=12)\n",
    "plt.title('Curva ROC - Comparaci√≥n de Modelos', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà AN√ÅLISIS DE LA CURVA ROC\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"AUC Modelo Inicial: {roc_auc_initial:.4f}\")\n",
    "print(f\"AUC Modelo Optimizado: {roc_auc_final:.4f}\")\n",
    "print(f\"Mejora en AUC: {roc_auc_final - roc_auc_initial:.4f}\")\n",
    "\n",
    "# Interpretaci√≥n del AUC\n",
    "if roc_auc_final >= 0.9:\n",
    "    interpretacion = \"Excelente\"\n",
    "elif roc_auc_final >= 0.8:\n",
    "    interpretacion = \"Bueno\"\n",
    "elif roc_auc_final >= 0.7:\n",
    "    interpretacion = \"Aceptable\"\n",
    "elif roc_auc_final >= 0.6:\n",
    "    interpretacion = \"Pobre\"\n",
    "else:\n",
    "    interpretacion = \"Muy pobre\"\n",
    "\n",
    "print(f\"\\nInterpretaci√≥n del AUC ({roc_auc_final:.4f}): {interpretacion}\")\n",
    "\n",
    "# Crear visualizaci√≥n interactiva con Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# A√±adir curva ROC del modelo inicial\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=fpr_initial, \n",
    "    y=tpr_initial,\n",
    "    mode='lines',\n",
    "    name=f'Modelo Inicial (AUC = {roc_auc_initial:.3f})',\n",
    "    line=dict(color='orange', width=3)\n",
    "))\n",
    "\n",
    "# A√±adir curva ROC del modelo optimizado\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=fpr_final, \n",
    "    y=tpr_final,\n",
    "    mode='lines',\n",
    "    name=f'Modelo Optimizado (AUC = {roc_auc_final:.3f})',\n",
    "    line=dict(color='blue', width=3)\n",
    "))\n",
    "\n",
    "# A√±adir l√≠nea diagonal\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], \n",
    "    y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Clasificador Aleatorio (AUC = 0.500)',\n",
    "    line=dict(color='red', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Curva ROC Interactiva - Comparaci√≥n de Modelos',\n",
    "    xaxis_title='Tasa de Falsos Positivos (1 - Especificidad)',\n",
    "    yaxis_title='Tasa de Verdaderos Positivos (Sensibilidad)',\n",
    "    showlegend=True,\n",
    "    width=700,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc481e4",
   "metadata": {},
   "source": [
    "## 12. Visualizaci√≥n del √Årbol de Decisi√≥n\n",
    "Visualizamos la estructura del √°rbol entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n del √°rbol de decisi√≥n\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(dt_final, \n",
    "          feature_names=X.columns, \n",
    "          class_names=['Clase 0', 'Clase 1'],\n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          fontsize=10,\n",
    "          max_depth=3)  # Limitamos la profundidad para mejor visualizaci√≥n\n",
    "\n",
    "plt.title('Estructura del √Årbol de Decisi√≥n Optimizado\\n(Limitado a 3 niveles para visualizaci√≥n)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Informaci√≥n sobre el √°rbol\n",
    "print(\"üå≥ INFORMACI√ìN DEL √ÅRBOL DE DECISI√ìN\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Profundidad del √°rbol: {dt_final.get_depth()}\")\n",
    "print(f\"N√∫mero de hojas: {dt_final.get_n_leaves()}\")\n",
    "print(f\"N√∫mero total de nodos: {dt_final.tree_.node_count}\")\n",
    "\n",
    "# Crear una versi√≥n simplificada del √°rbol para mejor comprensi√≥n\n",
    "dt_simple = DecisionTreeClassifier(\n",
    "    criterion=dt_final.criterion,\n",
    "    max_depth=3,  # √Årbol m√°s simple para visualizaci√≥n\n",
    "    min_samples_split=dt_final.min_samples_split,\n",
    "    min_samples_leaf=dt_final.min_samples_leaf,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt_simple.fit(X_train, y_train)\n",
    "\n",
    "# Visualizaci√≥n del √°rbol simplificado\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt_simple, \n",
    "          feature_names=X.columns, \n",
    "          class_names=['Clase 0', 'Clase 1'],\n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          fontsize=12)\n",
    "\n",
    "plt.title('√Årbol de Decisi√≥n Simplificado (Profundidad = 3)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparar rendimiento del √°rbol simplificado\n",
    "y_pred_simple = dt_simple.predict(X_test)\n",
    "accuracy_simple = accuracy_score(y_test, y_pred_simple)\n",
    "\n",
    "print(f\"\\nüìä COMPARACI√ìN DE COMPLEJIDAD:\")\n",
    "print(f\"√Årbol Optimizado - Accuracy: {accuracy_final:.4f}, Profundidad: {dt_final.get_depth()}\")\n",
    "print(f\"√Årbol Simplificado - Accuracy: {accuracy_simple:.4f}, Profundidad: {dt_simple.get_depth()}\")\n",
    "print(f\"Trade-off interpretabilidad vs rendimiento: {accuracy_final - accuracy_simple:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a466818",
   "metadata": {},
   "source": [
    "## 13. An√°lisis de Importancia de Variables\n",
    "Analizamos qu√© caracter√≠sticas son m√°s importantes para las predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c10045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la importancia de las caracter√≠sticas\n",
    "feature_importance = dt_final.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Crear dataframe con la importancia\n",
    "importance_df = pd.DataFrame({\n",
    "    'Caracter√≠stica': feature_names,\n",
    "    'Importancia': feature_importance\n",
    "}).sort_values('Importancia', ascending=False)\n",
    "\n",
    "print(\"üéØ IMPORTANCIA DE LAS CARACTER√çSTICAS\")\n",
    "print(\"=\" * 50)\n",
    "print(importance_df)\n",
    "\n",
    "# Visualizaci√≥n horizontal de importancia\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(range(len(feature_names)), \n",
    "                importance_df['Importancia'], \n",
    "                color=plt.cm.viridis(np.linspace(0, 1, len(feature_names))))\n",
    "\n",
    "plt.yticks(range(len(feature_names)), importance_df['Caracter√≠stica'])\n",
    "plt.xlabel('Importancia')\n",
    "plt.title('Importancia de las Caracter√≠sticas - √Årbol de Decisi√≥n', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualizaci√≥n con gr√°fico circular para caracter√≠sticas m√°s importantes\n",
    "# Solo mostrar caracter√≠sticas con importancia > 5%\n",
    "important_features = importance_df[importance_df['Importancia'] > 0.05]\n",
    "\n",
    "if len(important_features) > 0:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(important_features)))\n",
    "    \n",
    "    wedges, texts, autotexts = plt.pie(important_features['Importancia'], \n",
    "                                       labels=important_features['Caracter√≠stica'],\n",
    "                                       autopct='%1.1f%%',\n",
    "                                       colors=colors,\n",
    "                                       startangle=90,\n",
    "                                       explode=[0.05] * len(important_features))\n",
    "    \n",
    "    plt.title('Distribuci√≥n de Importancia de Caracter√≠sticas (>5%)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# An√°lisis detallado\n",
    "print(f\"\\nüìä AN√ÅLISIS DETALLADO DE IMPORTANCIA:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Caracter√≠stica m√°s importante: {importance_df.iloc[0]['Caracter√≠stica']} ({importance_df.iloc[0]['Importancia']:.4f})\")\n",
    "print(f\"Suma de importancias (debe ser 1.0): {importance_df['Importancia'].sum():.4f}\")\n",
    "\n",
    "# Top 3 caracter√≠sticas\n",
    "top_3 = importance_df.head(3)\n",
    "print(f\"\\nTop 3 caracter√≠sticas m√°s importantes:\")\n",
    "for i, row in top_3.iterrows():\n",
    "    print(f\"{i+1}. {row['Caracter√≠stica']}: {row['Importancia']:.4f} ({row['Importancia']*100:.1f}%)\")\n",
    "\n",
    "# Caracter√≠sticas poco importantes\n",
    "low_importance = importance_df[importance_df['Importancia'] < 0.01]\n",
    "if len(low_importance) > 0:\n",
    "    print(f\"\\nCaracter√≠sticas con baja importancia (<1%):\")\n",
    "    for _, row in low_importance.iterrows():\n",
    "        print(f\"- {row['Caracter√≠stica']}: {row['Importancia']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3b9bb",
   "metadata": {},
   "source": [
    "## 14. Conclusiones y Recomendaciones\n",
    "\n",
    "### Resumen del Proyecto\n",
    "\n",
    "En este proyecto hemos desarrollado un an√°lisis predictivo completo utilizando √°rboles de decisi√≥n, siguiendo todas las fases del aprendizaje autom√°tico:\n",
    "\n",
    "#### ‚úÖ **Fases Completadas:**\n",
    "\n",
    "1. **Exploraci√≥n de Datos**: An√°lisis estad√≠stico descriptivo y visualizaciones\n",
    "2. **Limpieza**: Detecci√≥n de duplicados y outliers\n",
    "3. **Preprocesamiento**: Codificaci√≥n de variables categ√≥ricas\n",
    "4. **Divisi√≥n de Datos**: Split estratificado (80% entrenamiento, 20% prueba)\n",
    "5. **Modelado**: Implementaci√≥n de √Årbol de Decisi√≥n\n",
    "6. **Optimizaci√≥n**: Validaci√≥n cruzada k-fold y GridSearchCV\n",
    "7. **Evaluaci√≥n**: M√©tricas completas y matriz de confusi√≥n\n",
    "8. **Visualizaciones**: Curva ROC, estructura del √°rbol e importancia de caracter√≠sticas\n",
    "\n",
    "#### üìä **Resultados Clave:**\n",
    "\n",
    "- **Modelo Optimizado vs Inicial**: Mejora significativa en rendimiento\n",
    "- **Curva ROC**: AUC que indica la capacidad discriminatoria del modelo\n",
    "- **Validaci√≥n Cruzada**: Evaluaci√≥n robusta del rendimiento\n",
    "- **Importancia de Caracter√≠sticas**: Identificaci√≥n de variables m√°s predictivas\n",
    "\n",
    "#### üîç **Consideraciones Importantes:**\n",
    "\n",
    "1. **Interpretabilidad vs Rendimiento**: Los √°rboles m√°s simples son m√°s interpretables\n",
    "2. **Overfitting**: La validaci√≥n cruzada ayuda a detectar sobreajuste\n",
    "3. **Selecci√≥n de Caracter√≠sticas**: Las variables de mayor importancia pueden guiar futuros an√°lisis\n",
    "\n",
    "#### üìù **Pr√≥ximos Pasos Recomendados:**\n",
    "\n",
    "1. **Probar otros algoritmos** (Random Forest, XGBoost) para comparar rendimiento\n",
    "2. **Ingenier√≠a de caracter√≠sticas** adicional basada en el an√°lisis de importancia\n",
    "3. **Validaci√≥n externa** con nuevos datos cuando est√©n disponibles\n",
    "4. **Implementaci√≥n en producci√≥n** con monitoreo del rendimiento del modelo\n",
    "\n",
    "---\n",
    "\n",
    "**Nota**: Este notebook utiliza datos sint√©ticos para demostraci√≥n. Para un proyecto real, sustituye la secci√≥n de carga de datos con tu dataset espec√≠fico."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
